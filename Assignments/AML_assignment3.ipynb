{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Machine Learning - programming assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please fill in:**\n",
    "* Ernö Groeneweg (4279662)\n",
    "* Otto Mättas (6324363)\n",
    "* Vincent de Wit (6970877)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement learning with function approximation\n",
    "In this assignment, you'll design an agent to complete an episodic task. The agent will be looking at a small part of the UU logo, and will have to decide which of the four compass directions to move in, to find the goal in the center as soon as possible.\n",
    "\n",
    "The following code defines various aspects of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-fc874a8af0de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnotebook\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;31m# For progress bars\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import layers, models, optimizers\n",
    "from tqdm.notebook import tqdm # For progress bars\n",
    "\n",
    "# Action space\n",
    "ACTION_NORTH = 0\n",
    "ACTION_EAST = 1\n",
    "ACTION_SOUTH = 2\n",
    "ACTION_WEST = 3\n",
    "ACTIONS = [ACTION_NORTH, ACTION_EAST, ACTION_SOUTH, ACTION_WEST]\n",
    "\n",
    "# Constants defining the environment\n",
    "GOAL = (300, 276)\n",
    "AVG_MOVEMENT_SIZE = 24\n",
    "\n",
    "RADIUS = 72\n",
    "BOUNDARY_WEST = GOAL[0] - RADIUS\n",
    "BOUNDARY_EAST = GOAL[0] + RADIUS\n",
    "BOUNDARY_NORTH = GOAL[1] - RADIUS\n",
    "BOUNDARY_SOUTH = GOAL[1] + RADIUS\n",
    "\n",
    "WINDOW_SIZE = 28\n",
    "\n",
    "TIME_LIMIT = 200\n",
    "TIMEOUT_REWARD = -10.0\n",
    "\n",
    "im = plt.imread(\"UU_logo_EN_CMYK.png\")\n",
    "\n",
    "# Convert to one color channel (using only the red channel), with white background\n",
    "im = im[:,:,0] * im[:,:,3] + (1.0 - im[:,:,3])\n",
    "\n",
    "# Get a \"camera view\" at the position indicated by state\n",
    "# Use reshape=True to format the output as a data point for the neural network\n",
    "def get_window(state, reshape=False):\n",
    "    # When indexing the image as an array, switch the coordinates: im[state[1], state[0]]\n",
    "    window = im[(state[1]-14):(state[1]+14), (state[0]-14):(state[0]+14)]\n",
    "    if reshape:\n",
    "        return np.reshape(window, (1, 28, 28, 1))\n",
    "    else:\n",
    "        return window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following three images show:\n",
    "* The original image, with a red dot marking the goal and a blue rectangle marking the area where the center of agent must remain. A movement that would take the agent outside this rectangle, places him at the boundary instead.\n",
    "* What the agent sees if he is exactly at the goal.\n",
    "* The part of the image the agent might see (this is slightly bigger than the blue box, because of the camera's size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(im[:,:], cmap='gray', vmin=0, vmax=1.0)\n",
    "# Plotting uses reversed y-axis now: larger y values are further down\n",
    "plt.plot(GOAL[0], GOAL[1], 'ro', linewidth=5)\n",
    "plt.plot([BOUNDARY_WEST, BOUNDARY_WEST, BOUNDARY_EAST, BOUNDARY_EAST, BOUNDARY_WEST],\n",
    "        [BOUNDARY_NORTH, BOUNDARY_SOUTH, BOUNDARY_SOUTH, BOUNDARY_NORTH, BOUNDARY_NORTH],\n",
    "        'b-')\n",
    "plt.show()\n",
    "\n",
    "# window around goal\n",
    "plt.imshow(get_window(GOAL), cmap='gray', vmin=0, vmax=1.0)\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(im[(BOUNDARY_NORTH-14):(BOUNDARY_SOUTH+14),(BOUNDARY_WEST-14):(BOUNDARY_EAST+14)], cmap='gray', vmin=0, vmax=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions complete the definition of the environment. The agent's movements always go in the intended direction, but the distance travelled has a small random component.\n",
    "\n",
    "Besides by reaching the goal, the episode also terminates after TIME_LIMIT (200) steps; at that point, the agent gets reward TIMEOUT_REWARD (-10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take an action in a state\n",
    "# Returns: new state, reward (always -1)\n",
    "def step(state, action):\n",
    "    x, y = state\n",
    "    if action == ACTION_NORTH:\n",
    "        y -= AVG_MOVEMENT_SIZE + np.random.randint(5) - 2\n",
    "        if y < BOUNDARY_NORTH:\n",
    "            y = BOUNDARY_NORTH\n",
    "    elif action == ACTION_SOUTH:\n",
    "        y += AVG_MOVEMENT_SIZE + np.random.randint(5) - 2\n",
    "        if y > BOUNDARY_SOUTH:\n",
    "            y = BOUNDARY_SOUTH\n",
    "    elif action == ACTION_WEST:\n",
    "        x -= AVG_MOVEMENT_SIZE + np.random.randint(5) - 2\n",
    "        if x < BOUNDARY_WEST:\n",
    "            x = BOUNDARY_WEST\n",
    "    elif action == ACTION_EAST:\n",
    "        x += AVG_MOVEMENT_SIZE + np.random.randint(5) - 2\n",
    "        if x > BOUNDARY_EAST:\n",
    "            x = BOUNDARY_EAST\n",
    "        \n",
    "    reward = -1.0\n",
    "    return (x, y), reward\n",
    "\n",
    "# Is the state close enough to the goal to be considered a success?\n",
    "# There is a margin for error, so that the agent can't jump over the goal\n",
    "def is_goal_reached(state):\n",
    "    if np.amax(np.abs(np.asarray(state) - np.asarray(GOAL))) <= AVG_MOVEMENT_SIZE / 2 + 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# Return a randomly chosen non-terminal state as starting state\n",
    "def starting_state():\n",
    "    while True:\n",
    "        state = (np.random.randint(BOUNDARY_WEST, BOUNDARY_EAST + 1),\n",
    "                 np.random.randint(BOUNDARY_NORTH, BOUNDARY_SOUTH + 1))\n",
    "        if not is_goal_reached(state):\n",
    "            return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class handles the function approximation, with several methods to query and update it. A neural network is used, similar to what can be used for MNIST digit recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueFunction:\n",
    "    def __init__(self, step_size):\n",
    "        self.step_size = step_size\n",
    "\n",
    "        # Use a small convolutional neural network to process the image\n",
    "        # The network has four output nodes: one for each action\n",
    "        # The final layer is initialized to zero, so that all value estimates are initially zero\n",
    "        self.model = models.Sequential()\n",
    "        self.model.add(layers.Conv2D(32, (5, 5), activation='relu',\n",
    "                                     input_shape=(28, 28, 1), data_format='channels_last'))\n",
    "        self.model.add(layers.MaxPooling2D((2, 2)))\n",
    "        self.model.add(layers.Conv2D(64, (5, 5), activation='relu'))\n",
    "        self.model.add(layers.MaxPooling2D((2, 2)))\n",
    "        self.model.add(layers.Flatten())\n",
    "        self.model.add(layers.Dense(64, activation='relu'))\n",
    "        self.model.add(layers.Dense(4, kernel_initializer='zeros', activation='linear'))\n",
    "        self.model.compile(loss='mean_squared_error', optimizer=optimizers.SGD(lr=step_size))\n",
    "\n",
    "    # Return estimated action value of given state and action\n",
    "    def value(self, state, action):\n",
    "        if is_goal_reached(state):\n",
    "            return 0.0\n",
    "        # Get value estimates for this state combined with each of the four actions\n",
    "        q = self.model.predict(get_window(state, reshape=True))\n",
    "        if not np.all(np.isfinite(q)):\n",
    "            print(\"* value() found nonfinite prediction:\", q)\n",
    "        # Return the value estimate for the requested (state, action) pair\n",
    "        return q[0, action]\n",
    "\n",
    "    # Return vector of estimated action values of given state, for each action\n",
    "    def values(self, state):\n",
    "        if is_goal_reached(state):\n",
    "            return np.zeros(4)\n",
    "        # Get value estimates for this state combined with each of the four actions\n",
    "        q = self.model.predict(get_window(state, reshape=True))\n",
    "        if not np.all(np.isfinite(q)):\n",
    "            print(\"* values() found nonfinite prediction:\", q)\n",
    "        # Return the value estimate for the requested (state, action) pair\n",
    "        return q[0, :]\n",
    "\n",
    "    # learn with given state, action and target\n",
    "    def learn(self, state, action, target):\n",
    "        if not np.all(np.isfinite(target)):\n",
    "            print(\"* learn() received nonfinite target:\", target)\n",
    "        # Get value estimates for this state combined with each of the four actions\n",
    "        window = get_window(state, reshape=True)\n",
    "        q = self.model.predict(window)\n",
    "        if not np.all(np.isfinite(q)):\n",
    "            print(\"* learn() found nonfinite prediction:\", q)\n",
    "        q[0, action] = target\n",
    "        self.model.train_on_batch(window, q)\n",
    "\n",
    "    # Return estimated state value, based on the estimated action values\n",
    "    def state_value(self, state):\n",
    "        return np.max(self.values(state))\n",
    "\n",
    "# Plot the state value estimates. Use a larger stride for lower resolution.\n",
    "def plot_state_values(value_function, stride=1):\n",
    "    v = np.zeros(((BOUNDARY_SOUTH - BOUNDARY_NORTH + stride) // stride,\n",
    "                  (BOUNDARY_EAST - BOUNDARY_WEST + stride) // stride))\n",
    "    for i, y in enumerate(range(BOUNDARY_NORTH, BOUNDARY_SOUTH + 1, stride)):\n",
    "        for j, x in enumerate(range(BOUNDARY_WEST, BOUNDARY_EAST + 1, stride)):\n",
    "            v[i,j] = value_function.state_value((x, y))\n",
    "    plt.imshow(v)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next comes your part. The following two functions are responsible for the agent's behavior. First, get_action should implement the epsilon-greedy policy, and return an action chosen according to that policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose action at state based on epsilon-greedy policy and valueFunction\n",
    "import random\n",
    "\n",
    "def get_action(state, value_function, epsilon):\n",
    "    # Your code here\n",
    "    draw = random.uniform(0, 1)\n",
    "    \n",
    "    if draw > epsilon:\n",
    "    # greedy    \n",
    "        values = value_function.value(state)\n",
    "        choice = max(values)\n",
    "        return choice\n",
    "    \n",
    "    else:\n",
    "    #random    \n",
    "        while(True):\n",
    "            choice = random.uniform(0,4)\n",
    "            if choice >= 0 and choice < 1:\n",
    "                return ACTION_NORTH\n",
    "            elif choice >= 1 and choice < 2:\n",
    "                return ACTION_EAST\n",
    "            elif choice >= 2 and choice < 3:\n",
    "                return ACTION_SOUTH\n",
    "            elif choice >=3 and choice < 4:\n",
    "                return ACTION_WEST\n",
    "        \n",
    "    return ACTION_NORTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function handles the interaction between agent and environment for a single episode. By passing the same value_function object in multiple calls to this function, the agent can learn from a sequence of such interactions.\n",
    "\n",
    "Implement the missing parts of semi-gradient n-step Q-learning. This algorithm is almost exactly the same as the semi-gradient n-step Sarsa algorithm in section 10.2 of the book, except that the final term of $G$ should be computed as for Q-learning rather than as for Sarsa.\n",
    "\n",
    "The main missing part is the computation of the update target. To be able to compute this, and to make the right call to value_function.learn, you'll also need to add code in other places in the function to keep track of previous states, actions and rewards. For full points, do this in a way that uses an amount of memory that depends on $n$, but not on the number of time steps the episode takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semi-gradient n-step Q-learning with approximation\n",
    "# valueFunction: action value function to learn\n",
    "# n: number of steps for multi-step TD\n",
    "# epsilon: parameter of epsilon-greedy policy                       \n",
    "# learning: should the value function be updated during the simulation?\n",
    "def semi_gradient_n_step_q_learning(value_function, n=1, epsilon=0.0, learning=True):\n",
    "    current_state = starting_state()\n",
    "    # get initial action\n",
    "    current_action = get_action(current_state, value_function, epsilon)\n",
    "\n",
    "    # track the time\n",
    "    time = 0\n",
    "\n",
    "    # termination time\n",
    "    T = float('inf')\n",
    "    \n",
    "    total_reward = 0.0\n",
    "    \n",
    "    while True:\n",
    "        if time < T:\n",
    "            # take current action and go to the new state\n",
    "            new_state, reward = step(current_state, current_action)\n",
    "\n",
    "            if is_goal_reached(new_state):\n",
    "                T = time + 1\n",
    "            elif time >= TIME_LIMIT:\n",
    "                T = time + 1\n",
    "                reward = TIMEOUT_REWARD\n",
    "            else:\n",
    "                # choose new action\n",
    "                new_action = get_action(new_state, value_function, epsilon)\n",
    "            \n",
    "            total_reward += reward\n",
    "\n",
    "        # get the time index of the state to update\n",
    "        update_time = time - n + 1\n",
    "        if update_time >= 0 and learning:\n",
    "            target = 0.0\n",
    "            \n",
    "            # Your code here\n",
    "            \n",
    "            # update the state value function\n",
    "            #value_function.learn(?, ?, target) # Fill in the blanks\n",
    "        if update_time == T - 1:\n",
    "            break\n",
    "        # go to next time step\n",
    "        time += 1\n",
    "        if time < T:\n",
    "            current_state = new_state\n",
    "            current_action = new_action\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to test the performance of the algorithm on the environment. Plot estimates of the return as a function of the episode, for a few combinations of the parameters $n$ and $\\alpha$. A reasonable starting point for $\\alpha$ is 1e-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also use your code to estimate the return of an agent that picks all actions uniformly at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the parameter values for which you saw the best performance. Are you satisfied with its performance? Use the plot_state_values function to get an idea of what is going on. Describe your observations in a few sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final note: In section 16.5 of the book, you'll find a description of the algorithm that learned to play ATARI games. At the core, it is the algorithm we used here, but with several additional techniques to improve its performance. Although we do not cover those techniques in this course or use them in this programming assignment, I hope this assignment has given you a better appreciation some of the issues that arise in reinforcement learning with function approximation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
